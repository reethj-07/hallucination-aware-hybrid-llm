# ğŸ§  Hallucination-Aware Hybrid LLM System
### Production-Grade RAG with Phi-3 and FAISS Retrieval

[![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python&logoColor=white)](https://www.python.org/)
[![Framework](https://img.shields.io/badge/FastAPI-0.111.0-green?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![Model](https://img.shields.io/badge/Phi--3-Mini-yellow)](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
[![Retrieval](https://img.shields.io/badge/FAISS-1.8-blue)](https://github.com/facebookresearch/faiss)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen)](#-quickstart)

---

## ğŸš€ Overview

This project implements a **production-grade Retrieval-Augmented Generation (RAG) system** that prevents hallucinations through **strict context-grounding**. Responses are retrieved from a FAISS vector index and verified against source documents before returning to users.

**Key Innovation**: Two deployment modes:
- **Lightweight Mode** (`RAG_LIGHTWEIGHT=true`) â€” Fast retrieval + template generation (20-50ms, 200MB memory)
- **Full Mode** â€” Phi-3 LoRA with cross-encoder reranking (100-500ms, 8GB+ memory)

**Current Status**: ğŸŸ¢ Operational with **86.7% retrieval accuracy** on test queries

---

## âœ¨ Key Features

- **ğŸ” FAISS Vector Retrieval** â€” Fast, accurate document similarity search on normalized embeddings  
- **ğŸ“š Normalized Embeddings** â€” Inner-product similarity with L2 normalization for stable retrieval  
- **âš¡ Lightweight & Full Modes** â€” Choose between fast template generation or full LLM reasoning  
- **ğŸš¦ Async Inference Queue** â€” Bounded concurrency with configurable worker threads  
- **ğŸ“Š Cross-Encoder Reranking** â€” Optional LRU-cached reranking for improved precision  
- **ğŸ›¡ï¸ Hallucination Guards** â€” Multi-level safeguards: retrieval constraints + prompt engineering + token overlap verification  
- **ğŸ” Production Features** â€” Rate limiting, API key auth, structured logging with request IDs, Prometheus metrics  
- **ğŸ“ˆ Evaluation Framework** â€” EM/F1/citation precision metrics on regression dataset  
- **ğŸ³ Docker-Ready** â€” Separate API + UI services with docker-compose  
- **ğŸ“ Citations & Grounding** â€” Track which documents support each answer with span-level citations  

---

## ğŸ— System Architecture

```
Client Request
    â†“
FastAPI /query endpoint
    â†“
Middleware: Auth + Rate Limiting + Logging
    â†“
Async Inference Queue (bounded concurrency)
    â†“
RAG Pipeline:
  â”œâ”€ Load FAISS index (LRU cache)
  â”œâ”€ Embed query (SentenceTransformer, cached)
  â”œâ”€ Retrieve top-K documents (< 1ms)
  â”œâ”€ Optional: Cross-encoder reranking (LRU cache)
  â”œâ”€ Budget context to MAX_CONTEXT_CHARS
  â”œâ”€ [LIGHTWEIGHT] Template-based answer extraction
  â””â”€ [FULL] Phi-3 LoRA generation (first call: 60-90s, subsequent: 100-500ms)
    â†“
Citation generation (span-level grounding)
    â†“
Hallucination guard (token overlap verification)
    â†“
JSON response with metrics to client
```

### Key Design Decisions

1. **Normalized Embeddings**: Inner-product similarity instead of L2 distance for improved retrieval
2. **Context Budgeting**: 4000-char limit prevents token overflow in model context
3. **LRU Caching**: Models, embeddings, and reranker scores cached for 2048+ entries
4. **Two Deployment Modes**: Choose performance vs. full reasoning based on hardware

---

## ğŸ›¡ï¸ Hallucination Prevention Strategy

The system uses **three independent safeguards** to prevent hallucinations:

### 1ï¸âƒ£ Retrieval Constraint
- Query embedded using SentenceTransformer (all-MiniLM-L6-v2)
- Top-K relevant documents retrieved via FAISS inner-product search
- **Only retrieved documents** passed as context to LLM
- No parametric knowledge allowed during generation

### 2ï¸âƒ£ Prompt-Level Generation Constraints
Strict instructions in system prompt:
```
- Answer ONLY using the provided context
- Do NOT use prior knowledge
- If answer not in context, respond exactly:
  "Not found in retrieved documents"
```

### 3ï¸âƒ£ Token Overlap Verification
- Generated answer verified against retrieved documents
- If below threshold overlap, forces abstention
- Optional: Cross-encoder reranking validates retrieved relevance
- Citations track exact document spans used

---

## ğŸ“Š Evaluation Results

**Retrieval Quality (15 QA pairs):**
- âœ… Success Rate: **86.7%** (13/15 correct retrievals)
- âœ… Precision: **1.00** (100% of retrieved queries matched expected keywords)
- âœ… Recall: **0.21** (average keyword coverage)
- âš ï¸ Known Issue: False retrieval for out-of-domain queries (Q14-Q15)

**Live API Testing:**
| Query | Response | Latency | Status |
|-------|----------|---------|--------|
| "What is the rate limit?" | "1000 requests per minute..." | 20-50ms | âœ… |
| "What encryption is used?" | "AES-256 at-rest, TLS 1.3 in transit..." | 20-50ms | âœ… |

See [SESSION_SUMMARY.md](SESSION_SUMMARY.md) for detailed metrics and test results.

---

## ğŸš€ Quickstart

### Prerequisites
```bash
Python 3.12+
~500MB RAM (lightweight mode) or 8GB+ RAM (full mode)
```

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Build FAISS Index
```bash
# First time only (creates 41-doc index in seconds)
RAG_SKIP_CHUNKING=true RAG_RERANK=false python -m rag.ingest_docs

# Output:
# âœ… Encoded 41 documents
# âœ… RAG documents indexed successfully (41 chunks)
```

### 3. Start API (Lightweight Mode - Recommended)
```bash
RAG_LIGHTWEIGHT=true RAG_RERANK=false python -m uvicorn api.main:app --port 8000

# Server ready at http://localhost:8000
```

### 4. Test with Sample Query
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the rate limit for Standard tier?"}'
```

**Response** (~50ms):
```json
{
  "query": "What is the rate limit for Standard tier?",
  "answer": "Rate Limiting: Standard tier allows 1000 requests per minute...",
  "used_rag": true,
  "retrieved_documents": [
    "Rate Limiting: Standard tier allows 1000 requests per minute...",
    "..."
  ],
  "citations": []
}
```

### 5. Run Evaluation
```bash
# Fast retrieval-only evaluation (no LLM)
RAG_RERANK=false python -m scripts.evaluate_retrieval

# Output:
# Retrieval Success Rate: 86.7%
# Avg Precision: 1.00
# Results saved to retrieval_results.json
```

---

## ğŸ“ Project Structure

```
hallucination-aware-hybrid-llm/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                      # FastAPI server (537 lines)
â”‚   â”œâ”€â”€ metrics.py                   # Prometheus endpoint
â”‚   â””â”€â”€ middleware.py                # Auth, rate limiting, logging
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ rag_inference.py             # Full RAG + Phi-3 (268 lines)
â”‚   â”œâ”€â”€ rag_inference_lightweight.py # Fast template-based (150 lines)
â”‚   â”œâ”€â”€ ingest_docs.py               # Build FAISS index
â”‚   â”œâ”€â”€ pipeline.py                  # Pipeline orchestration
â”‚   â””â”€â”€ faiss_index/
â”‚       â”œâ”€â”€ index.faiss              # Vector index (62KB)
â”‚       â””â”€â”€ docs.pkl                 # Document metadata (11KB)
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ run_lora_inference.py        # Phi-3 + LoRA generation
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ evaluate_rag.py              # Full RAG evaluation
â”‚   â””â”€â”€ evaluate_retrieval.py        # Retrieval-only metrics
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â””â”€â”€ qa_pairs.jsonl           # 15 regression test QA pairs
â”‚   â”œâ”€â”€ rag_docs/                    # 41 source documents
â”‚   â””â”€â”€ finetune/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py                  # API tests
â”‚   â””â”€â”€ test_rag_pipeline.py         # RAG pipeline tests
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py             # UI (optional)
â”œâ”€â”€ docker-compose.yml               # Multi-service deployment
â”œâ”€â”€ Dockerfile                       # API image
â”œâ”€â”€ requirements.txt                 # All deps pinned
â”œâ”€â”€ SESSION_SUMMARY.md               # Detailed session notes
â””â”€â”€ README.md                        # This file
```

---

## âš™ï¸ Configuration

Set via environment variables:

```bash
# RAG Behavior
RAG_LIGHTWEIGHT=true              # Use fast template mode (recommended)
RAG_RERANK=false                  # Skip cross-encoder reranking
RAG_MAX_CONTEXT_CHARS=4000        # Max context length
RAG_TOP_K=3                       # Top K docs to return
RAG_SKIP_CHUNKING=true            # Skip chunking (for speed)
RAG_INDEX_PATH=rag/faiss_index/index.faiss
RAG_DOCS_PATH=rag/faiss_index/docs.pkl

# API & Server
REQUEST_TIMEOUT_S=20              # Request timeout
API_KEY=your_secret_key           # Require API key auth (optional)
RATE_LIMIT=60/minute              # Rate limit per IP
CORS_ALLOW_ORIGINS=*              # CORS allowed origins
LOG_LEVEL=INFO                    # Logging level

# Models
RAG_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
RAG_RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
BASE_MODEL_ID=microsoft/Phi-3-mini-4k-instruct
LORA_PATH=models/phi3_lora_final
```

---

## ğŸ›  Tech Stack

| Component | Technology |
|-----------|-----------|
| **Backend** | FastAPI 0.111.0, Uvicorn, Starlette |
| **LLM** | Microsoft Phi-3 Mini (4k) + LoRA (PEFT 0.10.0) |
| **Retrieval** | FAISS 1.8 (inner-product) |
| **Embeddings** | SentenceTransformer 2.7.0 (all-MiniLM-L6-v2) |
| **Reranking** | CrossEncoder 2.x (optional) |
| **Async** | asyncio, anyio, threading |
| **Observability** | Prometheus 0.20.0, structured JSON logging |
| **Rate Limiting** | SlowAPI 0.1.9 |
| **UI** | Streamlit (optional) |
| **Testing** | pytest, locust |
| **Deployment** | Docker, docker-compose |

---

## ğŸ“ˆ Performance

| Operation | Lightweight | Full Mode | Notes |
|-----------|------------|-----------|-------|
| Cold embedder load | ~2-3s | ~2-3s | One-time per process |
| Query embedding | 20-50ms | 20-50ms | Cached |
| FAISS search | <1ms | <1ms | Top-10 retrieval |
| Reranking (if enabled) | N/A | 50-200ms | LRU cached |
| Generation | N/A | 100-500ms | Phi-3 on CPU |
| **Total per query** | **20-50ms** | **150-700ms** | First call slower |

---

## ğŸ”„ API Endpoints

### `/query` (POST)
Retrieve documents and generate grounded response.

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is X?", "use_rag": true}'
```

**Response:**
```json
{
  "query": "What is X?",
  "answer": "...",
  "used_rag": true,
  "retrieved_documents": ["...", "..."],
  "citations": [{"start": 0, "end": 10, "doc_index": 0, "snippet": "..."}]
}
```

### `/health` (GET)
Health check endpoint.

```bash
curl http://localhost:8000/health
# {"status": "ok"}
```

### `/metrics/prometheus` (GET)
Prometheus metrics endpoint.

```bash
curl http://localhost:8000/metrics/prometheus
# http_request_latency_seconds{endpoint="/query",...} 0.052
# http_requests_total{endpoint="/query",...} 5
# inference_queue_size 0
```

---

## ğŸ³ Docker Deployment

### Build & Run with Docker Compose

```bash
docker compose build
docker compose up
```

**Services:**
- **API**: http://localhost:8000 (FastAPI)
- **UI**: http://localhost:8501 (Streamlit, optional)

### Environment Configuration

Create `.env` file:
```
RAG_LIGHTWEIGHT=true
RAG_RERANK=false
API_KEY=your_secret_key
RATE_LIMIT=100/minute
```

---

## ğŸ§ª Testing

### Retrieval Evaluation (Fast, No LLM)
```bash
RAG_RERANK=false python -m scripts.evaluate_retrieval
```

### Full RAG Evaluation (Requires Phi-3)
```bash
REQUEST_TIMEOUT_S=180 python -m scripts.evaluate_rag
```

### Load Testing
```bash
python -m locust -f locustfile.py --host=http://localhost:8000 -u 50 -r 5
# Open http://localhost:8089 in browser
```

### Unit Tests
```bash
pytest tests/
```

---

## ğŸ“ Example Outputs

### âœ… Correct Answer (Grounded)
```
Query: "What is the rate limit for Standard tier?"
Answer: "Rate Limiting: Standard tier allows 1000 requests per minute. 
Premium tier allows 10000 requests per minute. Enterprise tier has custom limits."
Status: âœ… Correct, grounded in retrieved documents
```

### âœ… Correct Abstention (Not in Docs)
```
Query: "What quantum computing features are available?"
Answer: "Not found in retrieved documents"
Status: âœ… Correct - system recognized out-of-domain query
```

### âš ï¸ False Retrieval (Known Issue)
```
Query: "Does the platform support blockchain smart contracts?"
Retrieved: [Irrelevant docs about security/compliance]
Status: âš ï¸ Known limitation - insufficient out-of-domain detection
Fix: Implement similarity threshold or explicit out-of-corpus detection
```

---

## ğŸ”® Future Improvements

- [ ] Adaptive similarity threshold for abstention
- [ ] Fine-tuned domain-specific embedder
- [ ] Explicit "not in corpus" detection in generation
- [ ] Confidence scoring per answer
- [ ] Knowledge graph-based retrieval
- [ ] Multi-hop reasoning
- [ ] Streaming response support
- [ ] Batch query processing
- [ ] Custom LoRA adapter selection

---

## ğŸ“š References

- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [Sentence-Transformers](https://www.sbert.net/)
- [FastAPI Guide](https://fastapi.tiangolo.com/)
- [Phi-3 Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
- [LoRA Fine-Tuning](https://github.com/microsoft/LoRA)

---

## ğŸ“„ License

This project is open source. See [LICENSE](LICENSE) for details.

---

## âœ‰ï¸ Session Notes

For detailed implementation notes, architecture decisions, and troubleshooting, see [SESSION_SUMMARY.md](SESSION_SUMMARY.md).

**Status Update**: System is operational with 86.7% retrieval accuracy. Ready for deployment and further optimization.
